{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, chi2\nfrom sklearn.datasets import load_iris\n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T07:01:02.460299Z","iopub.execute_input":"2023-09-15T07:01:02.460681Z","iopub.status.idle":"2023-09-15T07:01:05.197876Z","shell.execute_reply.started":"2023-09-15T07:01:02.460646Z","shell.execute_reply":"2023-09-15T07:01:05.196391Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Load dataset\ndata = load_iris()\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = data.target\n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T07:01:05.200315Z","iopub.execute_input":"2023-09-15T07:01:05.200880Z","iopub.status.idle":"2023-09-15T07:01:05.218731Z","shell.execute_reply.started":"2023-09-15T07:01:05.200824Z","shell.execute_reply":"2023-09-15T07:01:05.217387Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Method 1: Correlation-based Feature Selection (for classification)\n# Calculate feature-target correlation\ncorrelation_scores = X.corrwith(pd.Series(y, name=\"Target\"))\ncorrelation_scores = correlation_scores.abs().sort_values(ascending=False)\n# Select the top 'k' features based on correlation (e.g., top 3)\nk = 3\nselected_features_corr = correlation_scores.index[:k]\n\nprint(selected_features_corr)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T07:11:04.666409Z","iopub.execute_input":"2023-09-15T07:11:04.666897Z","iopub.status.idle":"2023-09-15T07:11:04.681264Z","shell.execute_reply.started":"2023-09-15T07:11:04.666856Z","shell.execute_reply":"2023-09-15T07:11:04.680193Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Index(['petal width (cm)', 'petal length (cm)', 'sepal length (cm)'], dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"# Method 2: Mutual Information-based Feature Selection (for classification)\n# Select the top 'k' features based on mutual information (e.g., top 3)\nk = 3\nselected_features_mi = SelectKBest(score_func=mutual_info_classif, k=k).fit(X, y)\nselected_features_mi = X.columns[selected_features_mi.get_support()]\n\nprint(selected_features_mi)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T07:11:16.481071Z","iopub.execute_input":"2023-09-15T07:11:16.481480Z","iopub.status.idle":"2023-09-15T07:11:16.509443Z","shell.execute_reply.started":"2023-09-15T07:11:16.481451Z","shell.execute_reply":"2023-09-15T07:11:16.508272Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Index(['sepal length (cm)', 'petal length (cm)', 'petal width (cm)'], dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"# Chi-square Test\n\nfrom sklearn.feature_selection import SelectKBest, chi2\n\nchi2_features = SelectKBest(chi2, k=3)\nX_kbest_features = chi2_features.fit_transform(X, y)\n\nprint(X.shape)\nprint(X_kbest_features.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T07:01:05.307526Z","iopub.execute_input":"2023-09-15T07:01:05.308299Z","iopub.status.idle":"2023-09-15T07:01:05.334716Z","shell.execute_reply.started":"2023-09-15T07:01:05.308254Z","shell.execute_reply":"2023-09-15T07:01:05.333456Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"(150, 4)\n(150, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Fisher's Score\n\n# Perform Fisher's Score feature selection\nk = 2  # Number of top features to select\nf_score_selector = SelectKBest(score_func=f_classif, k=k, )\nX_new = f_score_selector.fit_transform(X, y)\n\n# Get the indices of the selected features\nselected_feature_indices = f_score_selector.get_support(indices=True)\n\n# Print the selected features and their indices\nselected_features = [data.feature_names[i] for i in selected_feature_indices]\nprint(\"Selected Features:\")\nprint(selected_features)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T07:02:44.426576Z","iopub.execute_input":"2023-09-15T07:02:44.427062Z","iopub.status.idle":"2023-09-15T07:02:44.442405Z","shell.execute_reply.started":"2023-09-15T07:02:44.427024Z","shell.execute_reply":"2023-09-15T07:02:44.441104Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Selected Features:\n['petal length (cm)', 'petal width (cm)']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Missing Value Ration\n\nfrom sklearn.impute import SimpleImputer\n\n# Calculate the missing value ratio for each feature\nmissing_value_threshold = 0.3  # Set your desired threshold for missing values (e.g., 30%)\nmissing_value_ratio = X.isnull().mean()\n\n# Select features with missing value ratio below the threshold\nselected_features = X.columns[missing_value_ratio < missing_value_threshold]\n\n# Impute missing values if needed (e.g., using mean imputation)\nimputer = SimpleImputer(strategy='mean')\nX[selected_features] = imputer.fit_transform(X[selected_features])\n\n# Print the selected features\nprint(\"Selected Features:\")\nprint(selected_features)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T07:09:43.046668Z","iopub.execute_input":"2023-09-15T07:09:43.047080Z","iopub.status.idle":"2023-09-15T07:09:43.080307Z","shell.execute_reply.started":"2023-09-15T07:09:43.047048Z","shell.execute_reply":"2023-09-15T07:09:43.079247Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Selected Features:\nIndex(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',\n       'petal width (cm)'],\n      dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}